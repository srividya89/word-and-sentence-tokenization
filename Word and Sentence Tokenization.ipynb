{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94d28f9f-dd7f-442e-b9d3-903ed791886c",
   "metadata": {},
   "source": [
    "Word Tokenization:lets import the word token using word-tokenize module.After installing nltk,then from nltk.tokenize we import the word_tokenize module here,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca58900-d898-4015-8edc-56587aa8c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05cc7be3-55e3-414d-83e2-7ba787f1b4bc",
   "metadata": {},
   "source": [
    "define our text or import from other sources ,Lets have the simple text stored in a variable ‘text’ below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e492013-54b4-4321-83f9-ae51d72db18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"I am learning Natural language proessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f039c421-5314-4a01-8809-7c1d105cbdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the word-tokenize, then we will pass the text. \n",
    "#So it will take the text & then it will split into word by word.\n",
    "#Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1177a62-b98a-4bca-8546-aa5734738a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6679ca4b-8460-480b-985c-c254bf583138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Natural', 'language', 'proessing']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549dd22-bc38-4548-9843-951461793dd1",
   "metadata": {},
   "source": [
    "Sentence Tokenization:\n",
    "Importing sentence tokenize (sent_tokenize) is same like word tokenize (word_tokenize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd26e8c8-9ab0-4510-a1c7-e4bb72d95c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0df13cb0-e4ff-4a43-a7b5-4a1fc907e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "598bf990-6364-498f-9387-e7939b654c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text = \"Hello! How are you doing today? I hope everything is going well.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "655daeca-dc37-40d8-b07d-ff96fb7aa252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35da140f-9cc2-4c69-b449-70ae643fda2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'How are you doing today?', 'I hope everything is going well.']\n"
     ]
    }
   ],
   "source": [
    "# Print the tokenized sentences\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e22fcc-5118-4298-a25f-565a760b3e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f9084-c4dd-4a6d-bbb3-f929aad5f3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3a599-0697-4cff-92ac-a707bb85fd49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
